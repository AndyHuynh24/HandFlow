{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05 - Model Evaluation & Analysis\n",
                "\n",
                "This notebook provides detailed evaluation of trained models.\n",
                "\n",
                "## What This Notebook Covers\n",
                "1. Load trained models\n",
                "2. Per-class performance analysis\n",
                "3. Error analysis\n",
                "4. Inference speed benchmarking\n",
                "5. Model size comparison\n",
                "6. Production readiness assessment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import os\n",
                "import sys\n",
                "import time\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, precision_recall_curve,\n",
                "    roc_curve, auc, f1_score, precision_score, recall_score\n",
                ")\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "\n",
                "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
                "\n",
                "from handflow.models import load_data, GesturePredictor\n",
                "from handflow.features import FeatureEngineer\n",
                "from handflow.utils import load_config\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "print(\"‚úÖ Imports loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "config = load_config()\n",
                "\n",
                "MODELS_DIR = Path('../models')\n",
                "DATA_PATH = Path('../data/raw/MP_Data')\n",
                "if not DATA_PATH.exists():\n",
                "    DATA_PATH = Path('../ModelTraining/MP_Data')\n",
                "\n",
                "ACTIONS = config.right_hand_gestures\n",
                "\n",
                "print(f\"üìÅ Models: {MODELS_DIR}\")\n",
                "print(f\"üìÅ Data: {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List available models\n",
                "model_files = list(MODELS_DIR.glob('*.h5')) + list(MODELS_DIR.glob('*.tflite')) + list(MODELS_DIR.glob('*.onnx'))\n",
                "\n",
                "print(\"üì¶ Available models:\")\n",
                "for mf in model_files:\n",
                "    size_mb = mf.stat().st_size / 1024 / 1024\n",
                "    print(f\"   {mf.name}: {size_mb:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load main model\n",
                "MODEL_PATH = MODELS_DIR / 'right_action.h5'\n",
                "\n",
                "if MODEL_PATH.exists():\n",
                "    model = keras.models.load_model(MODEL_PATH)\n",
                "    print(f\"‚úÖ Loaded model: {MODEL_PATH}\")\n",
                "    model.summary()\n",
                "else:\n",
                "    print(f\"‚ùå Model not found: {MODEL_PATH}\")\n",
                "    print(\"   Run training first: python scripts/train.py --hand right\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Test Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and prepare data\n",
                "print(\"üì• Loading data...\")\n",
                "sequences, labels = load_data(DATA_PATH, ACTIONS, config.model.sequence_length)\n",
                "\n",
                "# Apply feature engineering\n",
                "config.features.velocity = True\n",
                "config.features.acceleration = True\n",
                "config.features.finger_angles = True\n",
                "config.features.hand_bbox_size = True\n",
                "\n",
                "engineer = FeatureEngineer(config)\n",
                "X = np.array([engineer.transform(seq) for seq in sequences])\n",
                "y = labels\n",
                "\n",
                "print(f\"   X shape: {X.shape}\")\n",
                "print(f\"   y shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use all data for evaluation (or split if preferred)\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "_, X_test, _, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y.argmax(axis=1)\n",
                ")\n",
                "\n",
                "print(f\"üìä Test set: {X_test.shape[0]} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions\n",
                "if 'model' in dir():\n",
                "    y_pred_proba = model.predict(X_test, verbose=0)\n",
                "    y_pred = y_pred_proba.argmax(axis=1)\n",
                "    y_true = y_test.argmax(axis=1)\n",
                "    \n",
                "    # Overall metrics\n",
                "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
                "    print(f\"\\nüìä Overall Performance:\")\n",
                "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
                "    print(f\"   Test Accuracy: {test_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification report\n",
                "print(\"\\nüìã Classification Report:\")\n",
                "print(classification_report(y_true, y_pred, target_names=ACTIONS, digits=4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Raw counts\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=ACTIONS, yticklabels=ACTIONS, ax=axes[0])\n",
                "axes[0].set_xlabel('Predicted')\n",
                "axes[0].set_ylabel('True')\n",
                "axes[0].set_title('Confusion Matrix (Counts)')\n",
                "\n",
                "# Normalized\n",
                "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
                "            xticklabels=ACTIONS, yticklabels=ACTIONS, ax=axes[1])\n",
                "axes[1].set_xlabel('Predicted')\n",
                "axes[1].set_ylabel('True')\n",
                "axes[1].set_title('Confusion Matrix (Normalized)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../docs/evaluation_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Per-Class Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Per-class metrics\n",
                "per_class_metrics = []\n",
                "\n",
                "for idx, action in enumerate(ACTIONS):\n",
                "    mask = y_true == idx\n",
                "    if mask.sum() == 0:\n",
                "        continue\n",
                "    \n",
                "    # Binary metrics for this class\n",
                "    y_true_binary = (y_true == idx).astype(int)\n",
                "    y_pred_binary = (y_pred == idx).astype(int)\n",
                "    \n",
                "    tp = ((y_true == idx) & (y_pred == idx)).sum()\n",
                "    fp = ((y_true != idx) & (y_pred == idx)).sum()\n",
                "    fn = ((y_true == idx) & (y_pred != idx)).sum()\n",
                "    tn = ((y_true != idx) & (y_pred != idx)).sum()\n",
                "    \n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    \n",
                "    per_class_metrics.append({\n",
                "        'Gesture': action,\n",
                "        'Samples': mask.sum(),\n",
                "        'Precision': precision,\n",
                "        'Recall': recall,\n",
                "        'F1-Score': f1,\n",
                "        'True Positives': tp,\n",
                "        'False Positives': fp,\n",
                "        'False Negatives': fn\n",
                "    })\n",
                "\n",
                "df_metrics = pd.DataFrame(per_class_metrics)\n",
                "df_metrics = df_metrics.sort_values('F1-Score', ascending=False)\n",
                "df_metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize per-class performance\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# F1 Score by class\n",
                "colors = ['green' if x > 0.9 else 'orange' if x > 0.7 else 'red' for x in df_metrics['F1-Score']]\n",
                "axes[0].barh(df_metrics['Gesture'], df_metrics['F1-Score'], color=colors)\n",
                "axes[0].set_xlabel('F1-Score')\n",
                "axes[0].set_title('F1-Score by Gesture Class')\n",
                "axes[0].axvline(0.9, color='green', linestyle='--', alpha=0.5, label='Good (0.9)')\n",
                "axes[0].axvline(0.7, color='orange', linestyle='--', alpha=0.5, label='Okay (0.7)')\n",
                "axes[0].legend()\n",
                "\n",
                "# Precision vs Recall\n",
                "axes[1].scatter(df_metrics['Precision'], df_metrics['Recall'], s=100)\n",
                "for _, row in df_metrics.iterrows():\n",
                "    axes[1].annotate(row['Gesture'], (row['Precision'], row['Recall']),\n",
                "                    textcoords='offset points', xytext=(5, 5), fontsize=9)\n",
                "axes[1].set_xlabel('Precision')\n",
                "axes[1].set_ylabel('Recall')\n",
                "axes[1].set_title('Precision vs Recall by Class')\n",
                "axes[1].set_xlim(0, 1.1)\n",
                "axes[1].set_ylim(0, 1.1)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../docs/per_class_analysis.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Error Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find misclassified samples\n",
                "errors = y_true != y_pred\n",
                "error_indices = np.where(errors)[0]\n",
                "\n",
                "print(f\"üìä Error Analysis:\")\n",
                "print(f\"   Total errors: {len(error_indices)} / {len(y_true)} ({len(error_indices)/len(y_true):.1%})\")\n",
                "\n",
                "# Common misclassifications\n",
                "error_pairs = []\n",
                "for idx in error_indices:\n",
                "    error_pairs.append((ACTIONS[y_true[idx]], ACTIONS[y_pred[idx]]))\n",
                "\n",
                "from collections import Counter\n",
                "common_errors = Counter(error_pairs).most_common(10)\n",
                "\n",
                "print(\"\\nüòµ Most Common Misclassifications:\")\n",
                "for (true_label, pred_label), count in common_errors:\n",
                "    print(f\"   {true_label} ‚Üí {pred_label}: {count} times\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confidence analysis for errors\n",
                "error_confidences = y_pred_proba[errors].max(axis=1)\n",
                "correct_confidences = y_pred_proba[~errors].max(axis=1)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Confidence distribution\n",
                "axes[0].hist(correct_confidences, bins=30, alpha=0.7, label='Correct', color='green')\n",
                "axes[0].hist(error_confidences, bins=30, alpha=0.7, label='Errors', color='red')\n",
                "axes[0].set_xlabel('Prediction Confidence')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_title('Confidence Distribution: Correct vs Errors')\n",
                "axes[0].legend()\n",
                "\n",
                "# Confidence vs accuracy at different thresholds\n",
                "thresholds = np.linspace(0.5, 0.99, 20)\n",
                "accuracies = []\n",
                "coverages = []\n",
                "\n",
                "for thresh in thresholds:\n",
                "    mask = y_pred_proba.max(axis=1) >= thresh\n",
                "    if mask.sum() > 0:\n",
                "        accuracies.append((y_true[mask] == y_pred[mask]).mean())\n",
                "        coverages.append(mask.mean())\n",
                "    else:\n",
                "        accuracies.append(np.nan)\n",
                "        coverages.append(0)\n",
                "\n",
                "axes[1].plot(thresholds, accuracies, 'b-o', label='Accuracy')\n",
                "axes[1].plot(thresholds, coverages, 'g-s', label='Coverage')\n",
                "axes[1].set_xlabel('Confidence Threshold')\n",
                "axes[1].set_ylabel('Rate')\n",
                "axes[1].set_title('Accuracy vs Coverage at Different Thresholds')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../docs/error_analysis.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference Speed Benchmarking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark inference speed\n",
                "def benchmark_model(model, X_sample, n_runs=100):\n",
                "    \"\"\"\n",
                "    Benchmark model inference speed.\n",
                "    \"\"\"\n",
                "    # Warm up\n",
                "    for _ in range(10):\n",
                "        model.predict(X_sample, verbose=0)\n",
                "    \n",
                "    # Benchmark\n",
                "    times = []\n",
                "    for _ in range(n_runs):\n",
                "        start = time.perf_counter()\n",
                "        model.predict(X_sample, verbose=0)\n",
                "        times.append((time.perf_counter() - start) * 1000)  # ms\n",
                "    \n",
                "    return np.array(times)\n",
                "\n",
                "# Single sample inference\n",
                "single_sample = X_test[:1]\n",
                "times = benchmark_model(model, single_sample)\n",
                "\n",
                "print(f\"\\n‚ö° Inference Speed (single sample):\")\n",
                "print(f\"   Mean: {times.mean():.2f} ms\")\n",
                "print(f\"   Median: {np.median(times):.2f} ms\")\n",
                "print(f\"   95th percentile: {np.percentile(times, 95):.2f} ms\")\n",
                "print(f\"   Max: {times.max():.2f} ms\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize inference time distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "axes[0].hist(times, bins=30, edgecolor='black')\n",
                "axes[0].axvline(times.mean(), color='red', linestyle='--', label=f'Mean: {times.mean():.2f}ms')\n",
                "axes[0].axvline(20, color='green', linestyle='--', label='Target: 20ms')\n",
                "axes[0].set_xlabel('Inference Time (ms)')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_title('Inference Time Distribution')\n",
                "axes[0].legend()\n",
                "\n",
                "# Box plot\n",
                "axes[1].boxplot(times)\n",
                "axes[1].set_ylabel('Inference Time (ms)')\n",
                "axes[1].set_title('Inference Time Box Plot')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Size Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare model sizes\n",
                "size_data = []\n",
                "\n",
                "for mf in MODELS_DIR.glob('*.*'):\n",
                "    if mf.suffix in ['.h5', '.tflite', '.onnx']:\n",
                "        size_mb = mf.stat().st_size / 1024 / 1024\n",
                "        size_data.append({\n",
                "            'Model': mf.stem,\n",
                "            'Format': mf.suffix[1:].upper(),\n",
                "            'Size (MB)': size_mb\n",
                "        })\n",
                "\n",
                "df_sizes = pd.DataFrame(size_data)\n",
                "print(\"üì¶ Model Sizes:\")\n",
                "print(df_sizes.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Production Readiness Assessment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"üìã PRODUCTION READINESS ASSESSMENT\")\n",
                "print(\"=\"*60)\n",
                "print()\n",
                "\n",
                "# Criteria\n",
                "accuracy_threshold = 0.90\n",
                "latency_threshold = 20  # ms\n",
                "size_threshold = 10  # MB\n",
                "\n",
                "# Check accuracy\n",
                "accuracy_ok = test_acc >= accuracy_threshold\n",
                "print(f\"‚úÖ Accuracy >= {accuracy_threshold:.0%}: {test_acc:.1%} {'‚úì' if accuracy_ok else '‚úó'}\")\n",
                "\n",
                "# Check latency\n",
                "latency_ok = times.mean() <= latency_threshold\n",
                "print(f\"‚úÖ Latency <= {latency_threshold}ms: {times.mean():.1f}ms {'‚úì' if latency_ok else '‚úó'}\")\n",
                "\n",
                "# Check size\n",
                "model_size = MODEL_PATH.stat().st_size / 1024 / 1024\n",
                "size_ok = model_size <= size_threshold\n",
                "print(f\"‚úÖ Size <= {size_threshold}MB: {model_size:.1f}MB {'‚úì' if size_ok else '‚úó'}\")\n",
                "\n",
                "# Per-class F1 check\n",
                "min_f1 = df_metrics['F1-Score'].min()\n",
                "f1_ok = min_f1 >= 0.7\n",
                "print(f\"‚úÖ Min F1 >= 0.7: {min_f1:.2f} {'‚úì' if f1_ok else '‚úó'}\")\n",
                "\n",
                "print()\n",
                "overall_ready = accuracy_ok and latency_ok and size_ok and f1_ok\n",
                "if overall_ready:\n",
                "    print(\"üéâ MODEL IS PRODUCTION READY!\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Model needs improvement before production.\")\n",
                "    if not accuracy_ok:\n",
                "        print(\"   ‚Üí Improve accuracy (more data, better architecture)\")\n",
                "    if not latency_ok:\n",
                "        print(\"   ‚Üí Reduce latency (quantization, smaller model)\")\n",
                "    if not size_ok:\n",
                "        print(\"   ‚Üí Reduce size (quantization, pruning)\")\n",
                "    if not f1_ok:\n",
                "        print(\"   ‚Üí Improve weak classes (more examples, class weights)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This evaluation provides:\n",
                "- Overall and per-class performance metrics\n",
                "- Error analysis to identify weak points\n",
                "- Latency benchmarking for real-time use\n",
                "- Production readiness checklist\n",
                "\n",
                "Use these insights to:\n",
                "1. Identify gestures that need more training data\n",
                "2. Set appropriate confidence thresholds\n",
                "3. Decide if quantization is needed for speed\n",
                "4. Confirm readiness for deployment"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}